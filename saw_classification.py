# -*- coding: utf-8 -*-
"""SAW_multi_classification의 사본

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zZrNZQkM75KZKcROqnLVHw9nYsGKDQmf

#import
"""

import tensorflow as tf
import keras
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import random
import os

from sklearn.metrics import precision_score , recall_score , confusion_matrix, classification_report

"""#data processing

data load
"""

IMAGE_PATH = os.getcwd() #전처리한 데이터 path

normal_data = np.load(IMAGE_PATH+"/normal_9000_30.npy")
wet_data = np.load(IMAGE_PATH+"/wet_9000_30.npy")
oiled_data = np.load(IMAGE_PATH+"/oiled_9000_30.npy")
rust_data = np.load(IMAGE_PATH+"/rust_9000_30.npy")

normal_data.shape, wet_data.shape, oiled_data.shape, rust_data.shape

"""labeling"""

"""
정상 용접 : 0, 
흡습된 플럭스 : 1, 
유분 도포 : 2, 
녹 미처리: 3
"""

normal_label = np.zeros(normal_data.shape[0],).astype(int)
wet_label = (np.zeros(wet_data.shape[0],)+1).astype(int)
oiled_label = (np.zeros(oiled_data.shape[0],)+2).astype(int)
rust_label = (np.zeros(rust_data.shape[0],)+3).astype(int)

"""data concat"""

dataset = np.concatenate((normal_data, wet_data, oiled_data, rust_data)) 
labelset = np.concatenate((normal_label, wet_label, oiled_label, rust_label)) 

shuffle_idx = np.arange(dataset.shape[0])
np.random.shuffle(shuffle_idx)
dataset, labelset = dataset[shuffle_idx], labelset[shuffle_idx]

dataset.shape, labelset.shape

"""data split"""

# train, valid, test 데이터셋 분할

def split_data(X, y, train_ratio, valid_ratio, test_ratio):
  len = X.shape[0]
  idx1, idx2 = int(train_ratio * len), int((train_ratio+valid_ratio)*len)
  train_X, train_y = X[:idx1], y[:idx1]
  valid_X, valid_y = X[idx1:idx2], y[idx1:idx2]
  test_X, test_y = X[idx2:], y[idx2:]

  return train_X, train_y, valid_X, valid_y, test_X, test_y

train_X, train_y, valid_X, valid_y, test_X, test_y  = split_data(dataset, labelset, 0.6, 0.2, 0.2)

train_X.shape, valid_X.shape, test_X.shape

# #이미지 확인

# plt.figure(figsize=(30, 6))
# for i in range(10):
#   plt.subplot(1, 10,i+1)
#   plt.xticks([])
#   plt.yticks([])
#   plt.grid(False)
#   plt.imshow(train_X[i], cmap='plasma',vmin=0.,vmax=1)
# plt.show()

"""# Compile & Fit"""

np.random.seed(42)
tf.random.set_seed(42)

# callback 함수 정의
# 초반에 validation loss의 진동이 커서 early_stopping의 patience 40으로 설정 

early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=40, restore_best_weights=True)
reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=20, min_lr=0.0001)

"""##Xception

### compile & fit
"""

base_model = keras.applications.xception.Xception(weights='imagenet', include_top=False)
avg = keras.layers.GlobalAveragePooling2D()(base_model.output)
# drop = keras.layers.Dropout(0.2)(avg)
output = keras.layers.Dense(4, activation="softmax")(avg)

model = keras.models.Model(inputs=base_model.input, outputs=output)

optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)
model.compile(loss="sparse_categorical_crossentropy", optimizer=optimizer,
              metrics=["accuracy"])
history = model.fit(train_X,train_y, batch_size=40, epochs=10000, callbacks = [early_stopping_cb, reduce_lr], validation_data=(valid_X, valid_y))

pd.DataFrame(history.history).plot(figsize=(18,10))
plt.grid(True)
plt.gca().set_ylim(0,1)

plt.show()

"""###evaluate"""

np.set_printoptions(formatter={'float_kind': lambda x: "{0:0.3f}".format(x)})

model.evaluate(test_X, test_y)

"""예측 확률 및 예측 값"""

test_pred = model.predict(test_X)
test_pred

test_label = test_pred.argmax(axis=1)
test_label

"""confusion matrix"""

print(confusion_matrix(test_y, test_pred))

precision = precision_score(test_y, test_pred,average= "macro")
recall = recall_score(test_y, test_pred,average= "macro")
print('precision: {0:.4f}, recall: {1:.4f}'.format(precision, recall))

"""classification_report"""

print(classification_report(test_y, test_pred, digits=3))

"""#Save"""

model.save("SAW_multi_classification.h5")